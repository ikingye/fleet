{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 Status Fleet is currently alpha quality and actively being developed. Fleet is GitOps at scale. Fleet is designed to manage up to a million clusters. It's also lightweight enought that is works great for a single cluster too, but it really shines when you get to a large scale. By large scale we mean either a lot of clusters, a lot of deployments, or a lot of teams in a single organization. Fleet can manage deployments from git of raw Kubernetes YAML, Helm charts, or Kustomize or any combination of the three. Regardless of the source all resources are dynamically turned into Helm charts and Helm is used as the engine to deploy everything in the cluster. This give a high degree of control, consistency, and auditability. Fleet focuses not only on the ability to scale, but to give one a high degree of control and visibility to exactly what is installed on the cluster.","title":"Introduction"},{"location":"#introduction","text":"Status Fleet is currently alpha quality and actively being developed. Fleet is GitOps at scale. Fleet is designed to manage up to a million clusters. It's also lightweight enought that is works great for a single cluster too, but it really shines when you get to a large scale. By large scale we mean either a lot of clusters, a lot of deployments, or a lot of teams in a single organization. Fleet can manage deployments from git of raw Kubernetes YAML, Helm charts, or Kustomize or any combination of the three. Regardless of the source all resources are dynamically turned into Helm charts and Helm is used as the engine to deploy everything in the cluster. This give a high degree of control, consistency, and auditability. Fleet focuses not only on the ability to scale, but to give one a high degree of control and visibility to exactly what is installed on the cluster.","title":"Introduction"},{"location":"agent-initiated/","text":"Agent Initiated \u00b6 Refer to the overview page for a background information on the agent initiated registration style. Cluster Registration Token and Client ID \u00b6 An downstream cluster is registered using two pieces of information, the cluster registration token and the client ID . The cluster registration token is a credential that will authorize the downstream cluster agent to be able to initiate the registration process. Refer to the cluster registration token page for more information on how to create tokens and obtain the values. The cluster registration token is manifested as a values.yaml file that will be passed to the helm install process. The client ID is a unique string that will identify the cluster. This string is user generated and opaque to the Fleet manager and agent. It is only assumed to be sufficiently unique. For security reason one should probably not be able to easily guess this value as then one cluster could impersonate another. The client ID is optional and if not specific the UID field of the kube-system namespace resource will be used as the client ID. Upon registration if the client ID is found on a Cluster resource in the Fleet manager it will associate the agent with that Cluster . If no Cluster resource is found with that client ID a new Cluster resource will be created with the specific client ID. Client IDs are mostly important such that when a cluster is registered it can immediately be identified, assigned labels, and git repos can be deployed to it. Install agent for registration \u00b6 The Fleet agent is installed as a Helm chart. The only parameters to the helm chart installation should be the cluster registration token, which is represented by the values.yaml file and the client ID. The client ID is optional. First follow the cluster registration token page to obtain the values.yaml file to be used. Second setup your environment to use use a client ID. # If no client ID is going to be used then leave the value blank CLUSTER_CLIENT_ID = \"a-unique-value-for-this-cluster\" Finally, install the agent using Helm. Use proper namespace and release name For the agent chart the namespace must be fleet-system and the release name fleet-agent Ensure you are installing to the right cluster Helm will use the default context in ${HOME}/.kube/config to deploy the agent. Use --kubeconfig and --kube-context to change which cluster Helm is installing to. helm -n fleet-system install --create-namespace --wait \\ --set clientID = \" ${ CLUSTER_CLIENT_ID } \" \\ --values values.yaml \\ fleet-agent https://github.com/rancher/fleet/releases/download/v0.3.0-alpha6/fleet-agent-0.3.0-alpha6.tgz The agent should now be deployed. You can check that status of the fleet pods by running the below commands. # Ensure kubectl is pointing to the right cluster kubectl -n fleet-system logs -l app = fleet-agent kubectl -n fleet-system get pods -l app = fleet-agent Additionally you should see a new cluster registered in the Fleet manager. Below is an example of checking that a new cluster was registered in the clusters namespace . Please ensure your ${HOME}/.kube/config is pointed to the Fleet manager to run this command. kubectl -n clusters get clusters.fleet.cattle.io NAME BUNDLES-READY NODES-READY SAMPLE-NODE LAST-SEEN STATUS cluster-ab13e54400f1 1/1 1/1 k3d-cluster2-server-0 2020-08-31T19:23:10Z","title":"Agent Initiated"},{"location":"agent-initiated/#agent-initiated","text":"Refer to the overview page for a background information on the agent initiated registration style.","title":"Agent Initiated"},{"location":"agent-initiated/#cluster-registration-token-and-client-id","text":"An downstream cluster is registered using two pieces of information, the cluster registration token and the client ID . The cluster registration token is a credential that will authorize the downstream cluster agent to be able to initiate the registration process. Refer to the cluster registration token page for more information on how to create tokens and obtain the values. The cluster registration token is manifested as a values.yaml file that will be passed to the helm install process. The client ID is a unique string that will identify the cluster. This string is user generated and opaque to the Fleet manager and agent. It is only assumed to be sufficiently unique. For security reason one should probably not be able to easily guess this value as then one cluster could impersonate another. The client ID is optional and if not specific the UID field of the kube-system namespace resource will be used as the client ID. Upon registration if the client ID is found on a Cluster resource in the Fleet manager it will associate the agent with that Cluster . If no Cluster resource is found with that client ID a new Cluster resource will be created with the specific client ID. Client IDs are mostly important such that when a cluster is registered it can immediately be identified, assigned labels, and git repos can be deployed to it.","title":"Cluster Registration Token and Client ID"},{"location":"agent-initiated/#install-agent-for-registration","text":"The Fleet agent is installed as a Helm chart. The only parameters to the helm chart installation should be the cluster registration token, which is represented by the values.yaml file and the client ID. The client ID is optional. First follow the cluster registration token page to obtain the values.yaml file to be used. Second setup your environment to use use a client ID. # If no client ID is going to be used then leave the value blank CLUSTER_CLIENT_ID = \"a-unique-value-for-this-cluster\" Finally, install the agent using Helm. Use proper namespace and release name For the agent chart the namespace must be fleet-system and the release name fleet-agent Ensure you are installing to the right cluster Helm will use the default context in ${HOME}/.kube/config to deploy the agent. Use --kubeconfig and --kube-context to change which cluster Helm is installing to. helm -n fleet-system install --create-namespace --wait \\ --set clientID = \" ${ CLUSTER_CLIENT_ID } \" \\ --values values.yaml \\ fleet-agent https://github.com/rancher/fleet/releases/download/v0.3.0-alpha6/fleet-agent-0.3.0-alpha6.tgz The agent should now be deployed. You can check that status of the fleet pods by running the below commands. # Ensure kubectl is pointing to the right cluster kubectl -n fleet-system logs -l app = fleet-agent kubectl -n fleet-system get pods -l app = fleet-agent Additionally you should see a new cluster registered in the Fleet manager. Below is an example of checking that a new cluster was registered in the clusters namespace . Please ensure your ${HOME}/.kube/config is pointed to the Fleet manager to run this command. kubectl -n clusters get clusters.fleet.cattle.io NAME BUNDLES-READY NODES-READY SAMPLE-NODE LAST-SEEN STATUS cluster-ab13e54400f1 1/1 1/1 k3d-cluster2-server-0 2020-08-31T19:23:10Z","title":"Install agent for registration"},{"location":"architecture/","text":"Architecture \u00b6 Fleet has two primary components. The Fleet manager and the cluster agents. These components work in a two-stage pull model. The Fleet manager will pull from git and the cluster agents will pull from the Fleet manager. Fleet Manager \u00b6 The Fleet manager is a set of Kubernetes controllers running in any standard Kubernetes cluster. The only API exposed by the Fleet manages is the Kubernetes API, there is no custom API for the fleet controller. Cluster Agents \u00b6 One cluster agent runs in each cluster and is responsible for talking to the Fleet manager. The only communication from cluster to Fleet manager is by this agent and all communication goes from the managed cluster to the Fleet manager. The fleet manager does not initiate connections to downstream clusters. This means managed clusters can run in private networks and behind NATs. The only requirement is the cluster agent needs to be able to communicate with the Kubernetes API of the cluster running the Fleet manager. The one exception to this is if you use the manager initiated cluster registration flow. This is not required, but an optional patter. The cluster agents are not assumed to have an \"always on\" connection. They will resume operation as soon as they can connect. Future enhancements will probably add the ability to schedule times of when the agent checks in, as it stands right now they will always attempt to connect. Security \u00b6 The Fleet manager dynamically creates service account, manages their RBAC and then gives the tokens to the downstream clusters. Clusters are registered by optionally expiring cluster registration tokens. The cluster registration token is used only during the registration process to generate a credential specific to that cluster. After the cluster credential is established the cluster \"forgets\" the cluster registration token. The service accounts given to the clusters only have privileges to list BundleDeployment in the namespace created specifically for that cluster. It can also update the status subresource of BundleDeployment and the status subresource of it's Cluster resource. Scalability \u00b6 Fleet is designed to scale up to 1 million clusters. There are more details to come here on how we expect to scale a Kubernetes controller based architecture to 100's of millions of objects and beyond.","title":"Architecture"},{"location":"architecture/#architecture","text":"Fleet has two primary components. The Fleet manager and the cluster agents. These components work in a two-stage pull model. The Fleet manager will pull from git and the cluster agents will pull from the Fleet manager.","title":"Architecture"},{"location":"architecture/#fleet-manager","text":"The Fleet manager is a set of Kubernetes controllers running in any standard Kubernetes cluster. The only API exposed by the Fleet manages is the Kubernetes API, there is no custom API for the fleet controller.","title":"Fleet Manager"},{"location":"architecture/#cluster-agents","text":"One cluster agent runs in each cluster and is responsible for talking to the Fleet manager. The only communication from cluster to Fleet manager is by this agent and all communication goes from the managed cluster to the Fleet manager. The fleet manager does not initiate connections to downstream clusters. This means managed clusters can run in private networks and behind NATs. The only requirement is the cluster agent needs to be able to communicate with the Kubernetes API of the cluster running the Fleet manager. The one exception to this is if you use the manager initiated cluster registration flow. This is not required, but an optional patter. The cluster agents are not assumed to have an \"always on\" connection. They will resume operation as soon as they can connect. Future enhancements will probably add the ability to schedule times of when the agent checks in, as it stands right now they will always attempt to connect.","title":"Cluster Agents"},{"location":"architecture/#security","text":"The Fleet manager dynamically creates service account, manages their RBAC and then gives the tokens to the downstream clusters. Clusters are registered by optionally expiring cluster registration tokens. The cluster registration token is used only during the registration process to generate a credential specific to that cluster. After the cluster credential is established the cluster \"forgets\" the cluster registration token. The service accounts given to the clusters only have privileges to list BundleDeployment in the namespace created specifically for that cluster. It can also update the status subresource of BundleDeployment and the status subresource of it's Cluster resource.","title":"Security"},{"location":"architecture/#scalability","text":"Fleet is designed to scale up to 1 million clusters. There are more details to come here on how we expect to scale a Kubernetes controller based architecture to 100's of millions of objects and beyond.","title":"Scalability"},{"location":"bundles/","text":"Understanding Bundles \u00b6 Each registered GitRepo that is monitored can produce one or more bundles . The bundle is a collection of resources that contains resource that will be deployed to one or more clusters and can be customized per target. This means the structure of the monitored git repository is the same as the structure of a bundle or a directory structure of multiple bundles. The location of the bundles is specified in the GitRepo type with the spec.bundleDirs field. By default the value is ./ . Directory structure \u00b6 The directory structure of a single bundle will look like the below depending on your use case. ./fleet.yaml # Bundle descriptor (optional) ./manifests/ # Directory for raw kubernetes YAML ./chart/ # Directory for an inline Helm Chart ./kustomize/ # Directory for kustomization resources ./overlays/ ${ OVERLAY_NAME } # Directory for customize raw Kubernetes YAML resources Bundle Rending \u00b6 One can choose between using raw Kubernetes YAML, Helm, Kustomize, or some combination of the three. Regardless of the strategy you use, you should understand all three approaches to understand the capabilities of the system. Refer to examples for specific examples. Render Pipeline \u00b6 A bundle has three types of resources in it. Plain kubernetes manifests are available in manifest/ . Chart files for a helm chart are available in chart/ . Finally, kustomize files are in kustomize/ . Each one of these directories and content types are optional but combined to create one helm chart that is to be deployed to the cluster. Each content type is optional, but regardless of what type of input is chosen the final asset is always a Helm chart. Helm is core to the architecture of Fleet, but this does not mean you are required to author Helm charts yourself. You can choose a pure Kubernetes YAML or Kustomize approach. Phase 1: Plain Kubernetes YAML \u00b6 Any resource that is found in manifests/ will be copied to the target chart in the chart/templates/ folder. This means these files can be plain YAML or have helm golang templating. Phase 2: Helm Chart generation \u00b6 The chart/ folder is expected to have Helm chart content in it. If this folder is not found then a chart will be generated on demand. This means a Chart.yaml will be created for you if not found. Since content from manifests/ is copied to chart/templates , one can deploy helm charts without knowning anything about helm, instead using an approach closer to kubectl apply . Phase 3: Kustomize Post Process \u00b6 After the Helm chart from phase 2 is rendered, Fleet is called as a post renderer to apply run kustomize. The kustomizeDir field from the target or overlays can be used to determine which kustomization.yaml is invoked. The objects generated by Helm are put into a field named ${kustomizeDir}/manifests.yaml and the kustomization.yaml found in kustomizeDir is dynamically modified to add manifests.yaml to the resources: list. fleet.yaml \u00b6 A bundle is defined as optionally containing a fleet.yaml file at it's root. Again, this file is optional, but for many multi-cluster use cases it is fairly essential. The structure of the fleet.yaml is essentially the same structure as the spec field of the Bundle custom resource definition. Below is a reference of all fields and how they are used. Depending on the which style (single-cluster or multi-cluster) and rendered (raw YAML, Helm, or Kustomize) you are using the fields may or may not be applicable. # Used to populate metadata.labels in the Bundle custom resource. The labels of # a bundle are important if you wish to use the BundleNamespaceMapping approach # for common configuration management. labels : custom : value # Used to populate metadata.annotations in the Bundle custom resource. Currently # there is no specific use of annotations by Fleet and is here just to allow the # users to add additional metadata to bundles. annotations : custom : value # Use a custom folder for plain Kubernetes YAML files. This can also refer to a # URL to download resource from. This uses Hashicorp's go-getter, so any support # source (http, git, S3) should work. # Default: manifests manifestsDir : ./manifests # Use a custom folder for kustomize resources. This can also refer to a URL to # download resource from, similar to the manifestDir field # Default: kustomize kustomizeDir : ./kustomize # Use a custom source for chart resources. This is commonly a URL pointing to # the chart tgz file. Similar to the the manifestDir field any go-getter URL # is supported. # Default: chart chart : ./chart # The default namespace to be applied to resources. This field is not used to # enforce or lock down the deployment to a specific namespace, but instead # provide the default value of the namespace field if one is not specified # in the manifests. If you wish to actually restrict the namespace use then # that should be done using the RBAC of the service account assigned to the # GitRepo # Default: default namespace : default # When resources are applied the system will wait for the resources to initially # become Ready. If the resources are not ready in this time frame the # application of resources fails and the bundle will stay in a WaitApplied state. # Default: 600 (10 minutes) timeoutSeconds : 600 # Default values to be based to Helm upon installation. The structure of this # field is the same structure that would be in the values.yaml file. # Default: null values : image : custom/value:latest # A paused bundle will not update downstream clusters but instead mark the bundle # as OutOfSync. On can the manually confirm that a bundle should be deployed to # the downstream clusters. # Default: false paused : false rolloutStrategy : # A number or percentage of clusters that can be unavailable during an update # of a bundle. This follows the same basic approach as a deployment rollout # strategy. # default: 10% maxUnavailable : 15% # A number or percentage of cluster partitions that can be unavailable during # an update of a bundle. # default: 0 maxUnavailablePartitions : 20% # A number of percentage of how to automatically partition clusters if not # specific partitioning strategy is configured. # default: 25% autoPartitionSize : 10% # A list of definitions of partitions. If any target clusters do not match # the configuration they are added to partitions at the end following the # autoPartitionSize. partitions : # A user friend name given to the partition used for Display (optional). # default: \"\" - name : canary # A number or percentage of clusters that can be unavailable in this # partition before this partition is treated as done. # default: 10% maxUnavailable : 10% # Selector matching cluster labels to include in this partition clusterSelector : matchLabels : env : prod # A cluster group name to include in this partition clusterGroup : agroup # Selector matching cluster group labels to include in this partition clusterGroupSelector : agroup # Targets are used to match clusters that resources should be configured for. # Each target can specify a series of overlays to apply customizations for # that cluster. Targets are evaluated in order and the first one to match is used targets : # The name of target. If not specified a default name of the format \"target000\" # will be used - name : prod # Override namespace namespace : newvalue # Override the base dir where the kustomization.yaml is found # Please note this directory is relative to ./kustomize kustomizedDir : production/ # Override the timeoutSeconds parameter timeoutSeconds : 5 # Merge in new values used by Helm. The merge logic follows the logic of how Helm # merges values, which is basically just a map merge and list are overwritten. values : custom : value # Overlays to be applied on this target in the specified order. The names # of the overlays correspond to the directory names in the ./overlays folder. overlays : - custom2 - custom3 # A selector used to match clusters. The structure is the standard # metav1.LabelSelector format. If clusterGroupSelector or clusterGroup is specified, # clusterSelector will be used only to further refine the selection after # clusterGroupSelector and clusterGroup is evaluated. clusterSelector : matchLabels : env : prod # A selector used to match cluster groups. clusterGroupSelector : matchLabels : region : us-east # A specific clusterGroup by name that will be selected clusterGroup : group1 Target Matching \u00b6 All clusters and cluster groups in the same namespace as the GitRepo / Bundle will be evaluated against all bundle targets. The targets list is evaluated one by one and the first target that matches is used for that bundle for that cluster. If no match is made, then no customizations will be applied. The superset of all valid targets for a bundle is set in the definition of the GitRepo . The target definitions in the bundle are only used to decide what configuration to apply to the target, where as the actual matching of whether a cluster should be deployed to or not is determined by the definition of the GitRepo . There are three approaches to matching clusters. One can use cluster selectors, cluster group selectors, or an explicit cluster group name. All criteria is additive so the final match is evaluated as \"clusterSelector && clusterGroupSelector && clusterGroup\". If any of the three have the default value it is dropped from the criteria. The default value is either null or \"\". It is important to realize that the value {} for a selector means \"match everything.\" # Match everything clusterSelector : {} # Selector ignored clusterSelector : null Resource Overlays and Patching \u00b6 A target references a series of overlays and those overlay can have resources in them. The resource overlay content uses a file name based approach. This is different from kustomize which uses a resource based approach. In kustomize the resource Group, Kind, Version, Name, and Namespace identify resources and are then merged or patched. For Fleet the overlay resources will override or patch content with a matching file name. # Base files manifests/deployment.yaml manifests/svc.yaml # Overlay files # The follow file we be added overlays/custom/configmap.yaml # The following file will replace manifests/svc.yaml overlays/custom/svc.yaml # The following file will patch manifest/deployment.yaml overlays/custom/deployment_patch.yaml A file named foo will replace a file called foo from the base resources or a previous overlay. In order to patch content a file the convention of adding _patch. (notice the trailing period) to the filename is used. The string _patch. will be replaced with . from the file name and that will be used as the target. For example deployment_patch.yaml will target deployment.yaml . The patch will be applied using JSON Merge, Strategic Merge Patch, or JSON Patch. Which strategy is used is based on the file content. Even though JSON strategies are used, the files can be written using YAML syntax.","title":"Understanding Bundles"},{"location":"bundles/#understanding-bundles","text":"Each registered GitRepo that is monitored can produce one or more bundles . The bundle is a collection of resources that contains resource that will be deployed to one or more clusters and can be customized per target. This means the structure of the monitored git repository is the same as the structure of a bundle or a directory structure of multiple bundles. The location of the bundles is specified in the GitRepo type with the spec.bundleDirs field. By default the value is ./ .","title":"Understanding Bundles"},{"location":"bundles/#directory-structure","text":"The directory structure of a single bundle will look like the below depending on your use case. ./fleet.yaml # Bundle descriptor (optional) ./manifests/ # Directory for raw kubernetes YAML ./chart/ # Directory for an inline Helm Chart ./kustomize/ # Directory for kustomization resources ./overlays/ ${ OVERLAY_NAME } # Directory for customize raw Kubernetes YAML resources","title":"Directory structure"},{"location":"bundles/#bundle-rending","text":"One can choose between using raw Kubernetes YAML, Helm, Kustomize, or some combination of the three. Regardless of the strategy you use, you should understand all three approaches to understand the capabilities of the system. Refer to examples for specific examples.","title":"Bundle Rending"},{"location":"bundles/#render-pipeline","text":"A bundle has three types of resources in it. Plain kubernetes manifests are available in manifest/ . Chart files for a helm chart are available in chart/ . Finally, kustomize files are in kustomize/ . Each one of these directories and content types are optional but combined to create one helm chart that is to be deployed to the cluster. Each content type is optional, but regardless of what type of input is chosen the final asset is always a Helm chart. Helm is core to the architecture of Fleet, but this does not mean you are required to author Helm charts yourself. You can choose a pure Kubernetes YAML or Kustomize approach.","title":"Render Pipeline"},{"location":"bundles/#phase-1-plain-kubernetes-yaml","text":"Any resource that is found in manifests/ will be copied to the target chart in the chart/templates/ folder. This means these files can be plain YAML or have helm golang templating.","title":"Phase 1: Plain Kubernetes YAML"},{"location":"bundles/#phase-2-helm-chart-generation","text":"The chart/ folder is expected to have Helm chart content in it. If this folder is not found then a chart will be generated on demand. This means a Chart.yaml will be created for you if not found. Since content from manifests/ is copied to chart/templates , one can deploy helm charts without knowning anything about helm, instead using an approach closer to kubectl apply .","title":"Phase 2: Helm Chart generation"},{"location":"bundles/#phase-3-kustomize-post-process","text":"After the Helm chart from phase 2 is rendered, Fleet is called as a post renderer to apply run kustomize. The kustomizeDir field from the target or overlays can be used to determine which kustomization.yaml is invoked. The objects generated by Helm are put into a field named ${kustomizeDir}/manifests.yaml and the kustomization.yaml found in kustomizeDir is dynamically modified to add manifests.yaml to the resources: list.","title":"Phase 3: Kustomize Post Process"},{"location":"bundles/#fleetyaml","text":"A bundle is defined as optionally containing a fleet.yaml file at it's root. Again, this file is optional, but for many multi-cluster use cases it is fairly essential. The structure of the fleet.yaml is essentially the same structure as the spec field of the Bundle custom resource definition. Below is a reference of all fields and how they are used. Depending on the which style (single-cluster or multi-cluster) and rendered (raw YAML, Helm, or Kustomize) you are using the fields may or may not be applicable. # Used to populate metadata.labels in the Bundle custom resource. The labels of # a bundle are important if you wish to use the BundleNamespaceMapping approach # for common configuration management. labels : custom : value # Used to populate metadata.annotations in the Bundle custom resource. Currently # there is no specific use of annotations by Fleet and is here just to allow the # users to add additional metadata to bundles. annotations : custom : value # Use a custom folder for plain Kubernetes YAML files. This can also refer to a # URL to download resource from. This uses Hashicorp's go-getter, so any support # source (http, git, S3) should work. # Default: manifests manifestsDir : ./manifests # Use a custom folder for kustomize resources. This can also refer to a URL to # download resource from, similar to the manifestDir field # Default: kustomize kustomizeDir : ./kustomize # Use a custom source for chart resources. This is commonly a URL pointing to # the chart tgz file. Similar to the the manifestDir field any go-getter URL # is supported. # Default: chart chart : ./chart # The default namespace to be applied to resources. This field is not used to # enforce or lock down the deployment to a specific namespace, but instead # provide the default value of the namespace field if one is not specified # in the manifests. If you wish to actually restrict the namespace use then # that should be done using the RBAC of the service account assigned to the # GitRepo # Default: default namespace : default # When resources are applied the system will wait for the resources to initially # become Ready. If the resources are not ready in this time frame the # application of resources fails and the bundle will stay in a WaitApplied state. # Default: 600 (10 minutes) timeoutSeconds : 600 # Default values to be based to Helm upon installation. The structure of this # field is the same structure that would be in the values.yaml file. # Default: null values : image : custom/value:latest # A paused bundle will not update downstream clusters but instead mark the bundle # as OutOfSync. On can the manually confirm that a bundle should be deployed to # the downstream clusters. # Default: false paused : false rolloutStrategy : # A number or percentage of clusters that can be unavailable during an update # of a bundle. This follows the same basic approach as a deployment rollout # strategy. # default: 10% maxUnavailable : 15% # A number or percentage of cluster partitions that can be unavailable during # an update of a bundle. # default: 0 maxUnavailablePartitions : 20% # A number of percentage of how to automatically partition clusters if not # specific partitioning strategy is configured. # default: 25% autoPartitionSize : 10% # A list of definitions of partitions. If any target clusters do not match # the configuration they are added to partitions at the end following the # autoPartitionSize. partitions : # A user friend name given to the partition used for Display (optional). # default: \"\" - name : canary # A number or percentage of clusters that can be unavailable in this # partition before this partition is treated as done. # default: 10% maxUnavailable : 10% # Selector matching cluster labels to include in this partition clusterSelector : matchLabels : env : prod # A cluster group name to include in this partition clusterGroup : agroup # Selector matching cluster group labels to include in this partition clusterGroupSelector : agroup # Targets are used to match clusters that resources should be configured for. # Each target can specify a series of overlays to apply customizations for # that cluster. Targets are evaluated in order and the first one to match is used targets : # The name of target. If not specified a default name of the format \"target000\" # will be used - name : prod # Override namespace namespace : newvalue # Override the base dir where the kustomization.yaml is found # Please note this directory is relative to ./kustomize kustomizedDir : production/ # Override the timeoutSeconds parameter timeoutSeconds : 5 # Merge in new values used by Helm. The merge logic follows the logic of how Helm # merges values, which is basically just a map merge and list are overwritten. values : custom : value # Overlays to be applied on this target in the specified order. The names # of the overlays correspond to the directory names in the ./overlays folder. overlays : - custom2 - custom3 # A selector used to match clusters. The structure is the standard # metav1.LabelSelector format. If clusterGroupSelector or clusterGroup is specified, # clusterSelector will be used only to further refine the selection after # clusterGroupSelector and clusterGroup is evaluated. clusterSelector : matchLabels : env : prod # A selector used to match cluster groups. clusterGroupSelector : matchLabels : region : us-east # A specific clusterGroup by name that will be selected clusterGroup : group1","title":"fleet.yaml"},{"location":"bundles/#target-matching","text":"All clusters and cluster groups in the same namespace as the GitRepo / Bundle will be evaluated against all bundle targets. The targets list is evaluated one by one and the first target that matches is used for that bundle for that cluster. If no match is made, then no customizations will be applied. The superset of all valid targets for a bundle is set in the definition of the GitRepo . The target definitions in the bundle are only used to decide what configuration to apply to the target, where as the actual matching of whether a cluster should be deployed to or not is determined by the definition of the GitRepo . There are three approaches to matching clusters. One can use cluster selectors, cluster group selectors, or an explicit cluster group name. All criteria is additive so the final match is evaluated as \"clusterSelector && clusterGroupSelector && clusterGroup\". If any of the three have the default value it is dropped from the criteria. The default value is either null or \"\". It is important to realize that the value {} for a selector means \"match everything.\" # Match everything clusterSelector : {} # Selector ignored clusterSelector : null","title":"Target Matching"},{"location":"bundles/#resource-overlays-and-patching","text":"A target references a series of overlays and those overlay can have resources in them. The resource overlay content uses a file name based approach. This is different from kustomize which uses a resource based approach. In kustomize the resource Group, Kind, Version, Name, and Namespace identify resources and are then merged or patched. For Fleet the overlay resources will override or patch content with a matching file name. # Base files manifests/deployment.yaml manifests/svc.yaml # Overlay files # The follow file we be added overlays/custom/configmap.yaml # The following file will replace manifests/svc.yaml overlays/custom/svc.yaml # The following file will patch manifest/deployment.yaml overlays/custom/deployment_patch.yaml A file named foo will replace a file called foo from the base resources or a previous overlay. In order to patch content a file the convention of adding _patch. (notice the trailing period) to the filename is used. The string _patch. will be replaced with . from the file name and that will be used as the target. For example deployment_patch.yaml will target deployment.yaml . The patch will be applied using JSON Merge, Strategic Merge Patch, or JSON Patch. Which strategy is used is based on the file content. Even though JSON strategies are used, the files can be written using YAML syntax.","title":"Resource Overlays and Patching"},{"location":"cluster-group/","text":"Cluster Groups \u00b6 Clusters in a namespace can be put into a cluster group. A cluster group is essentially a named selector. The only parameter for a cluster group is essentially the selector. When you get to a specific scale the only reasonable way to manage clusters is by defining clusters groups. Cluster groups serve the purpose of giving aggregated status of the deployments and then also a simpler way to manage targets. A cluster group create by create a ClusterGroup resource like below kind : ClusterGroup apiVersion : fleet.cattle.io/v1alpha1 metadata : name : production-group namespace : clusters spec : # This is the standard metav1.LabelSelector format to match clusters by labels selector : matchLabels : env : prod","title":"Cluster Groups"},{"location":"cluster-group/#cluster-groups","text":"Clusters in a namespace can be put into a cluster group. A cluster group is essentially a named selector. The only parameter for a cluster group is essentially the selector. When you get to a specific scale the only reasonable way to manage clusters is by defining clusters groups. Cluster groups serve the purpose of giving aggregated status of the deployments and then also a simpler way to manage targets. A cluster group create by create a ClusterGroup resource like below kind : ClusterGroup apiVersion : fleet.cattle.io/v1alpha1 metadata : name : production-group namespace : clusters spec : # This is the standard metav1.LabelSelector format to match clusters by labels selector : matchLabels : env : prod","title":"Cluster Groups"},{"location":"cluster-overview/","text":"Overview \u00b6 There are two specific styles to registering clusters. These styles will be referred to as agent initiated and manager initiated registration. Typically one would go with the agent initiated registration but there are specific use cases in which manager initiated is a better workflow. Agent Initiated Registration \u00b6 Agent initiated refers to a pattern in which the downstream cluster installs an agent with a cluster registration token and optionally a client ID. The cluster agent will then make a API request to the Fleet manager and initiate the registration process. Using this process the Manager will never make an outbound API request to the downstream clusters and will thus never need to have direct network access. The downstream cluster only needs to make outbound HTTPS calls to the manager. Manager Initiated Registration \u00b6 Manager initiated registration is a process in which you register an existing Kubernetes cluster with the Fleet manager and the Fleet manager will make an API call to the downstream cluster to deploy the agent. This style can place additional network access requirements because the Fleet manager must be able to communicate with the download cluster API server for the registration process. After the cluster is registered there is no further need for the manager to contact the downstream cluster API. This style is more compatible if you wish to manage the creation of all your Kubernetes clusters through GitOps using something like cluster-api or Rancher .","title":"Overview"},{"location":"cluster-overview/#overview","text":"There are two specific styles to registering clusters. These styles will be referred to as agent initiated and manager initiated registration. Typically one would go with the agent initiated registration but there are specific use cases in which manager initiated is a better workflow.","title":"Overview"},{"location":"cluster-overview/#agent-initiated-registration","text":"Agent initiated refers to a pattern in which the downstream cluster installs an agent with a cluster registration token and optionally a client ID. The cluster agent will then make a API request to the Fleet manager and initiate the registration process. Using this process the Manager will never make an outbound API request to the downstream clusters and will thus never need to have direct network access. The downstream cluster only needs to make outbound HTTPS calls to the manager.","title":"Agent Initiated Registration"},{"location":"cluster-overview/#manager-initiated-registration","text":"Manager initiated registration is a process in which you register an existing Kubernetes cluster with the Fleet manager and the Fleet manager will make an API call to the downstream cluster to deploy the agent. This style can place additional network access requirements because the Fleet manager must be able to communicate with the download cluster API server for the registration process. After the cluster is registered there is no further need for the manager to contact the downstream cluster API. This style is more compatible if you wish to manage the creation of all your Kubernetes clusters through GitOps using something like cluster-api or Rancher .","title":"Manager Initiated Registration"},{"location":"cluster-tokens/","text":"Cluster Registration Tokens \u00b6 Unneeded for Manager initiated registration For manager initiated registrations the token is managed by the Fleet manager and does not need to be manually created an obtained. For an agent initiated registration the downstream cluster must have a cluster registration token. Cluster registration tokens are used to establish a new identity for a cluster. Internally cluster registration tokens are managed by creating Kubernetes service accounts that have the permissions to create ClusterRegistrationRequest s within a specific namespace. Once the cluster is registered a new ServiceAccount is created for that cluster that is used as the unique identity of the cluster. The agent is designed to forget the cluster registration token after registration. While the agent will not maintain a reference to the cluster registration token after a successful registration please note that usually other system bootstrap scripts do. Since the cluster registration token is forgotten, if you need to re-register a cluster you must give the cluster a new registration token. Token TTL \u00b6 Cluster registration tokens can be reused by any cluster in a namespace. The tokens can be given a TTL such that it will expire after a specific time. Create a new Token \u00b6 First you must understand how namespaces are used in the Fleet manager as the ClusterRegistationToken is a namespaced type. The cluster registration tokens are managed with the ClusterRegistrationToken type. Create a new token with the below YAML. kind : ClusterRegistrationToken apiVersion : \"fleet.cattle.io/v1alpha1\" metadata : name : new-token namespace : clusters spec : # The number of seconds this token is valid after creation. A value <= 0 means infinite time. ttlSeconds : 604800 Obtaining Token Value (Agent values.yaml) \u00b6 The token value is the contents of a values.yaml file that is expected to be passed to helm install to install the Fleet agent on a downstream cluster. The token is stored in a Kubernetes secret referenced by the status.secretName field on the newly created ClusterRegistrationToken . In practice the secret name is always the same as the ClusterRegistrationToken name. The contents will be in the secret's data key values . To obtain the values.yaml content for the above example YAML one can run the following one-liner. kubectl -n clusters get secret new-token -o 'jsonpath={.data.values}' | base64 -d > values.yaml This values.yaml file can now be used repeatedly by clusters to register until the TTL expires.","title":"Cluster Registration Tokens"},{"location":"cluster-tokens/#cluster-registration-tokens","text":"Unneeded for Manager initiated registration For manager initiated registrations the token is managed by the Fleet manager and does not need to be manually created an obtained. For an agent initiated registration the downstream cluster must have a cluster registration token. Cluster registration tokens are used to establish a new identity for a cluster. Internally cluster registration tokens are managed by creating Kubernetes service accounts that have the permissions to create ClusterRegistrationRequest s within a specific namespace. Once the cluster is registered a new ServiceAccount is created for that cluster that is used as the unique identity of the cluster. The agent is designed to forget the cluster registration token after registration. While the agent will not maintain a reference to the cluster registration token after a successful registration please note that usually other system bootstrap scripts do. Since the cluster registration token is forgotten, if you need to re-register a cluster you must give the cluster a new registration token.","title":"Cluster Registration Tokens"},{"location":"cluster-tokens/#token-ttl","text":"Cluster registration tokens can be reused by any cluster in a namespace. The tokens can be given a TTL such that it will expire after a specific time.","title":"Token TTL"},{"location":"cluster-tokens/#create-a-new-token","text":"First you must understand how namespaces are used in the Fleet manager as the ClusterRegistationToken is a namespaced type. The cluster registration tokens are managed with the ClusterRegistrationToken type. Create a new token with the below YAML. kind : ClusterRegistrationToken apiVersion : \"fleet.cattle.io/v1alpha1\" metadata : name : new-token namespace : clusters spec : # The number of seconds this token is valid after creation. A value <= 0 means infinite time. ttlSeconds : 604800","title":"Create a new Token"},{"location":"cluster-tokens/#obtaining-token-value-agent-valuesyaml","text":"The token value is the contents of a values.yaml file that is expected to be passed to helm install to install the Fleet agent on a downstream cluster. The token is stored in a Kubernetes secret referenced by the status.secretName field on the newly created ClusterRegistrationToken . In practice the secret name is always the same as the ClusterRegistrationToken name. The contents will be in the secret's data key values . To obtain the values.yaml content for the above example YAML one can run the following one-liner. kubectl -n clusters get secret new-token -o 'jsonpath={.data.values}' | base64 -d > values.yaml This values.yaml file can now be used repeatedly by clusters to register until the TTL expires.","title":"Obtaining Token Value (Agent values.yaml)"},{"location":"concepts/","text":"Core Concepts \u00b6 Fleet is fundamentally a set of Kubernetes custom resource definitions (CRDs) and controllers to manage GitOps for a single Kubernetes cluster or a large scale deployments of Kubernetes clusters (up to one million). Below are some of the concepts of Fleet that will be useful through out this documentation. Fleet Manager : The centralized component that orchestrates the deployments of Kubernetes assets from git. In a multi-cluster setup this will typically be a dedicated Kubernetes cluster. In a single cluster setup the Fleet manager will be running on the same cluster you are managing with GitOps. Fleet controller : The controller(s) running on the Fleet manager orchestrating GitOps. In practice Fleet manager and Fleet controllers is used fairly interchangeably. Single Cluster Style : This is a style of installing Fleet in which the manager and downstream cluster are the same cluster. This is a very simple pattern to quickly get up and running with GitOps. Multi Cluster Style : This is a style of running Fleet in which you have a central manager that manages a large number of downstream clusters. Fleet agent : Every managed downstream cluster will run an agent that communicates back to the Fleet manager. This agent is just another set of Kubernetes controllers running in the downstream cluster. GitRepo : Git repositories that are watched by Fleet are represented by the type GitRepo . Bundle : When a GitRepo is scanned it will produce one or more bundles. Bundles are a collection of resources that get deployed to a cluster. Bundle is the fundamental deployment unit used in Fleet. The contents of a Bundle may be Kubernetes manifests, Kustomize configuration, or Helm charts. BundleDeployment : When a Bundle is deployed to a cluster an instance of a Bundle is called a BundleDeployment . A BundleDeployment represents the state of that Bundle on a specific cluster with it's cluster specific customizations. Downstream Cluster : Clusters to which Fleet deploys manifests are referred to as downstream clusters. In the single cluster use case the Fleet manager Kubernetes cluster is both the manager and downstream cluster at the same time. Cluster Registration Token : Tokens used by agents to register a new cluster.","title":"Core Concepts"},{"location":"concepts/#core-concepts","text":"Fleet is fundamentally a set of Kubernetes custom resource definitions (CRDs) and controllers to manage GitOps for a single Kubernetes cluster or a large scale deployments of Kubernetes clusters (up to one million). Below are some of the concepts of Fleet that will be useful through out this documentation. Fleet Manager : The centralized component that orchestrates the deployments of Kubernetes assets from git. In a multi-cluster setup this will typically be a dedicated Kubernetes cluster. In a single cluster setup the Fleet manager will be running on the same cluster you are managing with GitOps. Fleet controller : The controller(s) running on the Fleet manager orchestrating GitOps. In practice Fleet manager and Fleet controllers is used fairly interchangeably. Single Cluster Style : This is a style of installing Fleet in which the manager and downstream cluster are the same cluster. This is a very simple pattern to quickly get up and running with GitOps. Multi Cluster Style : This is a style of running Fleet in which you have a central manager that manages a large number of downstream clusters. Fleet agent : Every managed downstream cluster will run an agent that communicates back to the Fleet manager. This agent is just another set of Kubernetes controllers running in the downstream cluster. GitRepo : Git repositories that are watched by Fleet are represented by the type GitRepo . Bundle : When a GitRepo is scanned it will produce one or more bundles. Bundles are a collection of resources that get deployed to a cluster. Bundle is the fundamental deployment unit used in Fleet. The contents of a Bundle may be Kubernetes manifests, Kustomize configuration, or Helm charts. BundleDeployment : When a Bundle is deployed to a cluster an instance of a Bundle is called a BundleDeployment . A BundleDeployment represents the state of that Bundle on a specific cluster with it's cluster specific customizations. Downstream Cluster : Clusters to which Fleet deploys manifests are referred to as downstream clusters. In the single cluster use case the Fleet manager Kubernetes cluster is both the manager and downstream cluster at the same time. Cluster Registration Token : Tokens used by agents to register a new cluster.","title":"Core Concepts"},{"location":"examples/","text":"Examples \u00b6 Examples using raw Kubernetes YAML, Helm charts, Kustomize and combinations of the three are in the Fleet Examples repo .","title":"Examples"},{"location":"examples/#examples","text":"Examples using raw Kubernetes YAML, Helm charts, Kustomize and combinations of the three are in the Fleet Examples repo .","title":"Examples"},{"location":"gitrepo-add/","text":"Registering \u00b6 Proper namespace \u00b6 Git repos are added to the Fleet manager using the GitRepo custom resource type. The GitRepo type is namespaced. If you are using Fleet in a single cluster style the namespace will always be fleet-local . For a multi-cluster style please ensure you use the correct repo that will map to the right target clusters. Create GitRepo instance \u00b6 Git repositories are register by creating a GitRepo following the below YAML sample. Refer to the inline comments as the means of each field kind : GitRepo apiVersion : fleet.cattle.io/v1alpha1 metadata : # Any name can be used here, the created bundles will start with this name name : my-repo # For single cluster use fleet-local, otherwise use the namespace of # your choosing namespace : fleet-local spec : # This can be a HTTPS or git URL. If you are using a git URL then # clientSecretName will probably need to be set to supply a credential. # repo is the only required parameter for a repo to be monitored. # repo : https://github.com/rancher/fleet-examples # Any branch can be watched, this field is optional. If not specified the # branch is assumed to be master # # branch: master # A specific commit or tag can also be watched. # # revision: v0.3.0 # For a private registry you must supply a clientSecretName. A default # secret can be set at the namespace level using the BundleRestriction # type. Secrets must be of the type \"kubernetes.io/ssh-auth\" or # \"kubernetes.io/basic-auth\". The secret is assumed to be in the # same namespace as the GitRepo # # clientSecretName: my-ssh-key # A git repo can produce multiple bundles or maybe your bundle # is not at the root of the git repo. The below field is expected # to be an array of paths and supports path globbing (ex: some/*/path) # # Example: # bundleDirs: # - single-path # - multiple-paths/* bundleDirs : - simple # The service account that will be used to perform this deployment. # This is the name of the service account that exists in the # downstream cluster in the fleet-system namespace. It is assumed # this service account already exists so it should be create before # hand, most likely coming from another git repo registered with # the Fleet manager. # # serviceAccount: moreSecureAccountThanClusterAdmin # Target clusters to deploy to if running Fleet in a multi-cluster # style. Refer to the \"Mapping to Downstream Clusters\" docs for # more information. # # targets: ...","title":"Registering"},{"location":"gitrepo-add/#registering","text":"","title":"Registering"},{"location":"gitrepo-add/#proper-namespace","text":"Git repos are added to the Fleet manager using the GitRepo custom resource type. The GitRepo type is namespaced. If you are using Fleet in a single cluster style the namespace will always be fleet-local . For a multi-cluster style please ensure you use the correct repo that will map to the right target clusters.","title":"Proper namespace"},{"location":"gitrepo-add/#create-gitrepo-instance","text":"Git repositories are register by creating a GitRepo following the below YAML sample. Refer to the inline comments as the means of each field kind : GitRepo apiVersion : fleet.cattle.io/v1alpha1 metadata : # Any name can be used here, the created bundles will start with this name name : my-repo # For single cluster use fleet-local, otherwise use the namespace of # your choosing namespace : fleet-local spec : # This can be a HTTPS or git URL. If you are using a git URL then # clientSecretName will probably need to be set to supply a credential. # repo is the only required parameter for a repo to be monitored. # repo : https://github.com/rancher/fleet-examples # Any branch can be watched, this field is optional. If not specified the # branch is assumed to be master # # branch: master # A specific commit or tag can also be watched. # # revision: v0.3.0 # For a private registry you must supply a clientSecretName. A default # secret can be set at the namespace level using the BundleRestriction # type. Secrets must be of the type \"kubernetes.io/ssh-auth\" or # \"kubernetes.io/basic-auth\". The secret is assumed to be in the # same namespace as the GitRepo # # clientSecretName: my-ssh-key # A git repo can produce multiple bundles or maybe your bundle # is not at the root of the git repo. The below field is expected # to be an array of paths and supports path globbing (ex: some/*/path) # # Example: # bundleDirs: # - single-path # - multiple-paths/* bundleDirs : - simple # The service account that will be used to perform this deployment. # This is the name of the service account that exists in the # downstream cluster in the fleet-system namespace. It is assumed # this service account already exists so it should be create before # hand, most likely coming from another git repo registered with # the Fleet manager. # # serviceAccount: moreSecureAccountThanClusterAdmin # Target clusters to deploy to if running Fleet in a multi-cluster # style. Refer to the \"Mapping to Downstream Clusters\" docs for # more information. # # targets: ...","title":"Create GitRepo instance"},{"location":"gitrepo-rm/","text":"Removing \u00b6 If you delete a GitRepo from the Fleet Manager the Bundles created by the git repo are not automatically removed. This is to prevent accidentally deleting software from clusters by just modifying the git repos. To fully remove the deployed software just delete the corresponding bundles too. This can be done by running kubectl -n \" ${ REPO_NAMESPACE } \" delete bundles.fleet.cattle.io -l fleet.cattle.io/repo-name = \" ${ REPO_NAME } \"","title":"Removing"},{"location":"gitrepo-rm/#removing","text":"If you delete a GitRepo from the Fleet Manager the Bundles created by the git repo are not automatically removed. This is to prevent accidentally deleting software from clusters by just modifying the git repos. To fully remove the deployed software just delete the corresponding bundles too. This can be done by running kubectl -n \" ${ REPO_NAMESPACE } \" delete bundles.fleet.cattle.io -l fleet.cattle.io/repo-name = \" ${ REPO_NAME } \"","title":"Removing"},{"location":"gitrepo-structure/","text":"Expected Repo Structure \u00b6 A registered git repository should have the following structure ./fleet.yaml # Bundle descriptor (optional) ./manifests/ # Directory for raw kubernetes YAML (if used) ./chart/ # Directory for an inline Helm Chart (if used) ./kustomize/ # Directory for kustomization resources (if used) ./overlays/ ${ OVERLAY_NAME } # Directory for customize raw Kubernetes YAML resources (if used) These directories can be configured to different paths using the fleet.yaml file. Refer to the bundle reference documentation on how to customize the behavior. Also refer to the examples to learn how to use raw YAML, Helm, and Kustomize and how to customize deployments to specific clusters.","title":"Expected Repo Structure"},{"location":"gitrepo-structure/#expected-repo-structure","text":"A registered git repository should have the following structure ./fleet.yaml # Bundle descriptor (optional) ./manifests/ # Directory for raw kubernetes YAML (if used) ./chart/ # Directory for an inline Helm Chart (if used) ./kustomize/ # Directory for kustomization resources (if used) ./overlays/ ${ OVERLAY_NAME } # Directory for customize raw Kubernetes YAML resources (if used) These directories can be configured to different paths using the fleet.yaml file. Refer to the bundle reference documentation on how to customize the behavior. Also refer to the examples to learn how to use raw YAML, Helm, and Kustomize and how to customize deployments to specific clusters.","title":"Expected Repo Structure"},{"location":"gitrepo-targets/","text":"Mapping to Downstream Clusters \u00b6 Multi-cluster Only This approach only applies if you are running Fleet in a multi-cluster style When deploying GitRepos to downstream clusters the clusters must be mapped to a target. Defining targets \u00b6 The deployment targets of GitRepo is done using the spec.targets field to match clusters or cluster groups. The YAML specification is as below. kind : GitRepo apiVersion : fleet.cattle.io/v1alpha1 metadata : name : myrepo namespace : clusters spec : repo : http://github.com/rancher/fleet-examples bundleDirs : - simple # Targets are evaluated in order and the first one to match is used. If # no targets match then the evaluated cluster will not be deployed to. targets : # The name of target. If not specified a default name of the format \"target000\" # will be used - name : prod # A selector used to match clusters. The structure is the standard # metav1.LabelSelector format. If clusterGroupSelector or clusterGroup is specified, # clusterSelector will be used only to further refine the selection after # clusterGroupSelector and clusterGroup is evaluated. clusterSelector : matchLabels : env : prod # A selector used to match cluster groups. clusterGroupSelector : matchLabels : region : us-east # A specific clusterGroup by name that will be selected clusterGroup : group1 Target Matching \u00b6 All clusters and cluster groups in the same namespace as the GitRepo will be evaluated against all targets. If any of the targets match the cluster then the GitRepo will be deployed to the downstream cluster. If no match is made, then the GitRepo will not be deployed to that cluster. There are three approaches to matching clusters. One can use cluster selectors, cluster group selectors, or an explicit cluster group name. All criteria is additive so the final match is evaluated as \"clusterSelector && clusterGroupSelector && clusterGroup\". If any of the three have the default value it is dropped from the criteria. The default value is either null or \"\". It is important to realize that the value {} for a selector means \"match everything.\" # Match everything clusterSelector : {} # Selector ignored clusterSelector : null Default target \u00b6 If no target is set for the GitRepo then the default targets value is applied. The default targets value is as below. targets : - name : default clusterGroup : default This means if you wish to setup a default location non-configured GitRepos will go to, then just create a cluster group called default and add clusters to it.","title":"Mapping to Downstream Clusters"},{"location":"gitrepo-targets/#mapping-to-downstream-clusters","text":"Multi-cluster Only This approach only applies if you are running Fleet in a multi-cluster style When deploying GitRepos to downstream clusters the clusters must be mapped to a target.","title":"Mapping to Downstream Clusters"},{"location":"gitrepo-targets/#defining-targets","text":"The deployment targets of GitRepo is done using the spec.targets field to match clusters or cluster groups. The YAML specification is as below. kind : GitRepo apiVersion : fleet.cattle.io/v1alpha1 metadata : name : myrepo namespace : clusters spec : repo : http://github.com/rancher/fleet-examples bundleDirs : - simple # Targets are evaluated in order and the first one to match is used. If # no targets match then the evaluated cluster will not be deployed to. targets : # The name of target. If not specified a default name of the format \"target000\" # will be used - name : prod # A selector used to match clusters. The structure is the standard # metav1.LabelSelector format. If clusterGroupSelector or clusterGroup is specified, # clusterSelector will be used only to further refine the selection after # clusterGroupSelector and clusterGroup is evaluated. clusterSelector : matchLabels : env : prod # A selector used to match cluster groups. clusterGroupSelector : matchLabels : region : us-east # A specific clusterGroup by name that will be selected clusterGroup : group1","title":"Defining targets"},{"location":"gitrepo-targets/#target-matching","text":"All clusters and cluster groups in the same namespace as the GitRepo will be evaluated against all targets. If any of the targets match the cluster then the GitRepo will be deployed to the downstream cluster. If no match is made, then the GitRepo will not be deployed to that cluster. There are three approaches to matching clusters. One can use cluster selectors, cluster group selectors, or an explicit cluster group name. All criteria is additive so the final match is evaluated as \"clusterSelector && clusterGroupSelector && clusterGroup\". If any of the three have the default value it is dropped from the criteria. The default value is either null or \"\". It is important to realize that the value {} for a selector means \"match everything.\" # Match everything clusterSelector : {} # Selector ignored clusterSelector : null","title":"Target Matching"},{"location":"gitrepo-targets/#default-target","text":"If no target is set for the GitRepo then the default targets value is applied. The default targets value is as below. targets : - name : default clusterGroup : default This means if you wish to setup a default location non-configured GitRepos will go to, then just create a cluster group called default and add clusters to it.","title":"Default target"},{"location":"installation/","text":"Installation \u00b6 The installation is broken up into two different use cases: Single and Multi-Cluster install. The single cluster install is for if you wish to use GitOps to manage a single cluster, in which case you do not need a centralized manager cluster. The multi-cluster use case setup you will setup a centralized manager cluster to which you can register clusters. If you are just learning Fleet the single cluster install is recommend starting point and you can move from single cluster to multi-cluster down the line.","title":"Overview"},{"location":"installation/#installation","text":"The installation is broken up into two different use cases: Single and Multi-Cluster install. The single cluster install is for if you wish to use GitOps to manage a single cluster, in which case you do not need a centralized manager cluster. The multi-cluster use case setup you will setup a centralized manager cluster to which you can register clusters. If you are just learning Fleet the single cluster install is recommend starting point and you can move from single cluster to multi-cluster down the line.","title":"Installation"},{"location":"manager-initiated/","text":"Manager Initiated \u00b6 Refer to the overview page for a background information on the manager initiated registration style. TODO \u00b6","title":"Manager Initiated"},{"location":"manager-initiated/#manager-initiated","text":"Refer to the overview page for a background information on the manager initiated registration style.","title":"Manager Initiated"},{"location":"manager-initiated/#todo","text":"","title":"TODO"},{"location":"multi-cluster-install/","text":"Multi-cluster Install \u00b6 In this use case you will setup a centralized Fleet manager. The centralize Fleet manage is a Kubernetes cluster running the Fleet controllers. After installing the Fleet manager you will then need to register remote downstream clusters with the Fleet manager. Prerequisites \u00b6 Helm 3 \u00b6 Fleet is distributed as a Helm chart. Helm 3 is just a CLI and has no server side component so it's pretty straight forward. To install the Helm 3 CLI follow the official install instructions . The TL;DR is macOS brew install helm Windows choco install kubernetes-helm Kubernetes \u00b6 The Fleet manager is a controller running on a Kubernetes cluster so an existing cluster is required. All downstream cluster that will be managed will need to communicate to this central Kubernetes cluster. This means the Kubernetes API server URL must be accesible to the downstream clusters. Any Kubernetes community supported version of Kubernetes will work, in practice this means 1.15 or greater. API Server URL and CA certificate \u00b6 In order for your Fleet management installation to properly work it is important the correct API server URL and CA certificates are configured properly. The Fleet agents will communicate to the Kubernetes API server URL. This means the Kubernetes API server must be accessible to the downstream clusters. You will also need to obtain the CA certificate of the API server. The easiest way to obtain this information is typically from your kubeconfig file ( ${HOME}/.kube/config ). The server and certificate-authority fields will have these values. apiVersion : v1 clusters : - cluster : certificate-authority-data : LS0tLS1CRUdJTi... server : https://example.com:6443 Please note that the certificate-authority-data field is base64 encoded and will need to be decoded before you save it into a file. This can be done by saving the base64 encoded contents to a file and then run base64 -d encoded-file > ca.pem If you have jq and base64 available then this one-liners will pull all CA certificates from your ${HOME}/.kube/config and place then in a file named ca.pem . kubectl config view -o json --raw | jq -r '.clusters[].cluster[\"certificate-authority-data\"]' | base64 -d > ca.pem Install \u00b6 In the following example it will be assumed the API server URL is https://example.com:6443 and the CA certificate is in the file ca.pem . If your API server URL is signed by a well known CA you can omit the apiServerCA parameter below or just create an empty ca.pem file (ie touch ca.pem ). Run the following commands Setup the environment with your specific values. API_SERVER_URL = \"https://example.com:6443\" # Leave empty if your API server is signed by a well known CA API_SERVER_CA = \"ca.pem\" First validate the server URL is correct. curl -fLk ${ API_SERVER_URL } /version The output of this command should be JSON with the version of the Kubernetes server or a 401 Unauthorized error. If you do not get either of these results than please ensure you have the correct URL. The API server port is typically 6443 for Kubernetes. Next validate that the CA certificate is proper by running the below command. If your API server is signed by a well known CA then omit the --cacert ${API_SERVER_CA} part of the command. curl -fL --cacert ${ API_SERVER_CA } ${ API_SERVER_URL } /version If you get a valid JSON response or an 401 Unauthorized then it worked. The Unauthorized error is only because the curl command is not setting proper credentials, but this validates that the TLS connection work and the ca.pem is correct for this URL. If you get a SSL certificate problem then the ca.pem is not correct. The contents of the ${API_SERVER_CA} file should look similar to the below -----BEGIN CERTIFICATE----- MIIBVjCB/qADAgECAgEAMAoGCCqGSM49BAMCMCMxITAfBgNVBAMMGGszcy1zZXJ2 ZXItY2FAMTU5ODM5MDQ0NzAeFw0yMDA4MjUyMTIwNDdaFw0zMDA4MjMyMTIwNDda MCMxITAfBgNVBAMMGGszcy1zZXJ2ZXItY2FAMTU5ODM5MDQ0NzBZMBMGByqGSM49 AgEGCCqGSM49AwEHA0IABDXlQNkXnwUPdbSgGz5Rk6U9ldGFjF6y1YyF36cNGk4E 0lMgNcVVD9gKuUSXEJk8tzHz3ra/+yTwSL5xQeLHBl+jIzAhMA4GA1UdDwEB/wQE AwICpDAPBgNVHRMBAf8EBTADAQH/MAoGCCqGSM49BAMCA0cAMEQCIFMtZ5gGDoDs ciRyve+T4xbRNVHES39tjjup/LuN4tAgAiAteeB3jgpTMpZyZcOOHl9gpZ8PgEcN KDs/pb3fnMTtpA== -----END CERTIFICATE----- Once you have validated the API server URL and API server CA parameters, install the following two Helm charts. First install the Fleet CustomResourcesDefintions. helm -n fleet-system install --create-namespace --wait fleet-crd https://github.com/rancher/fleet/releases/download/v0.3.0-alpha6/fleet-crd-0.3.0-alpha6.tgz Second install the Fleet controllers. helm -n fleet-system install --create-namespace --wait \\ --set apiServerURL = \" ${ API_SERVER_URL } \" \\ --set-file apiServerCA = \" ${ API_SERVER_CA } \" \\ fleet https://github.com/rancher/fleet/releases/download/v0.3.0-alpha6/fleet-0.3.0-alpha6.tgz Fleet should be ready to use. You can check the status of the Fleet controller pods by running the below commands. kubectl -n fleet-system logs -l app = fleet-controller kubectl -n fleet-system get pods -l app = fleet-controller NAME READY STATUS RESTARTS AGE fleet-controller-64f49d756b-n57wq 1/1 Running 0 3m21s At this point the Fleet manager should be ready. You can now register clusters and git repos with the Fleet manager.","title":"Multi-cluster Install"},{"location":"multi-cluster-install/#multi-cluster-install","text":"In this use case you will setup a centralized Fleet manager. The centralize Fleet manage is a Kubernetes cluster running the Fleet controllers. After installing the Fleet manager you will then need to register remote downstream clusters with the Fleet manager.","title":"Multi-cluster Install"},{"location":"multi-cluster-install/#prerequisites","text":"","title":"Prerequisites"},{"location":"multi-cluster-install/#helm-3","text":"Fleet is distributed as a Helm chart. Helm 3 is just a CLI and has no server side component so it's pretty straight forward. To install the Helm 3 CLI follow the official install instructions . The TL;DR is macOS brew install helm Windows choco install kubernetes-helm","title":"Helm 3"},{"location":"multi-cluster-install/#kubernetes","text":"The Fleet manager is a controller running on a Kubernetes cluster so an existing cluster is required. All downstream cluster that will be managed will need to communicate to this central Kubernetes cluster. This means the Kubernetes API server URL must be accesible to the downstream clusters. Any Kubernetes community supported version of Kubernetes will work, in practice this means 1.15 or greater.","title":"Kubernetes"},{"location":"multi-cluster-install/#api-server-url-and-ca-certificate","text":"In order for your Fleet management installation to properly work it is important the correct API server URL and CA certificates are configured properly. The Fleet agents will communicate to the Kubernetes API server URL. This means the Kubernetes API server must be accessible to the downstream clusters. You will also need to obtain the CA certificate of the API server. The easiest way to obtain this information is typically from your kubeconfig file ( ${HOME}/.kube/config ). The server and certificate-authority fields will have these values. apiVersion : v1 clusters : - cluster : certificate-authority-data : LS0tLS1CRUdJTi... server : https://example.com:6443 Please note that the certificate-authority-data field is base64 encoded and will need to be decoded before you save it into a file. This can be done by saving the base64 encoded contents to a file and then run base64 -d encoded-file > ca.pem If you have jq and base64 available then this one-liners will pull all CA certificates from your ${HOME}/.kube/config and place then in a file named ca.pem . kubectl config view -o json --raw | jq -r '.clusters[].cluster[\"certificate-authority-data\"]' | base64 -d > ca.pem","title":"API Server URL and CA certificate"},{"location":"multi-cluster-install/#install","text":"In the following example it will be assumed the API server URL is https://example.com:6443 and the CA certificate is in the file ca.pem . If your API server URL is signed by a well known CA you can omit the apiServerCA parameter below or just create an empty ca.pem file (ie touch ca.pem ). Run the following commands Setup the environment with your specific values. API_SERVER_URL = \"https://example.com:6443\" # Leave empty if your API server is signed by a well known CA API_SERVER_CA = \"ca.pem\" First validate the server URL is correct. curl -fLk ${ API_SERVER_URL } /version The output of this command should be JSON with the version of the Kubernetes server or a 401 Unauthorized error. If you do not get either of these results than please ensure you have the correct URL. The API server port is typically 6443 for Kubernetes. Next validate that the CA certificate is proper by running the below command. If your API server is signed by a well known CA then omit the --cacert ${API_SERVER_CA} part of the command. curl -fL --cacert ${ API_SERVER_CA } ${ API_SERVER_URL } /version If you get a valid JSON response or an 401 Unauthorized then it worked. The Unauthorized error is only because the curl command is not setting proper credentials, but this validates that the TLS connection work and the ca.pem is correct for this URL. If you get a SSL certificate problem then the ca.pem is not correct. The contents of the ${API_SERVER_CA} file should look similar to the below -----BEGIN CERTIFICATE----- MIIBVjCB/qADAgECAgEAMAoGCCqGSM49BAMCMCMxITAfBgNVBAMMGGszcy1zZXJ2 ZXItY2FAMTU5ODM5MDQ0NzAeFw0yMDA4MjUyMTIwNDdaFw0zMDA4MjMyMTIwNDda MCMxITAfBgNVBAMMGGszcy1zZXJ2ZXItY2FAMTU5ODM5MDQ0NzBZMBMGByqGSM49 AgEGCCqGSM49AwEHA0IABDXlQNkXnwUPdbSgGz5Rk6U9ldGFjF6y1YyF36cNGk4E 0lMgNcVVD9gKuUSXEJk8tzHz3ra/+yTwSL5xQeLHBl+jIzAhMA4GA1UdDwEB/wQE AwICpDAPBgNVHRMBAf8EBTADAQH/MAoGCCqGSM49BAMCA0cAMEQCIFMtZ5gGDoDs ciRyve+T4xbRNVHES39tjjup/LuN4tAgAiAteeB3jgpTMpZyZcOOHl9gpZ8PgEcN KDs/pb3fnMTtpA== -----END CERTIFICATE----- Once you have validated the API server URL and API server CA parameters, install the following two Helm charts. First install the Fleet CustomResourcesDefintions. helm -n fleet-system install --create-namespace --wait fleet-crd https://github.com/rancher/fleet/releases/download/v0.3.0-alpha6/fleet-crd-0.3.0-alpha6.tgz Second install the Fleet controllers. helm -n fleet-system install --create-namespace --wait \\ --set apiServerURL = \" ${ API_SERVER_URL } \" \\ --set-file apiServerCA = \" ${ API_SERVER_CA } \" \\ fleet https://github.com/rancher/fleet/releases/download/v0.3.0-alpha6/fleet-0.3.0-alpha6.tgz Fleet should be ready to use. You can check the status of the Fleet controller pods by running the below commands. kubectl -n fleet-system logs -l app = fleet-controller kubectl -n fleet-system get pods -l app = fleet-controller NAME READY STATUS RESTARTS AGE fleet-controller-64f49d756b-n57wq 1/1 Running 0 3m21s At this point the Fleet manager should be ready. You can now register clusters and git repos with the Fleet manager.","title":"Install"},{"location":"namespaces/","text":"Namespaces \u00b6 All types in the Fleet manager are namespaced. The namespaces of the manager types do not correspond to the namespaces of the deployed resources in the downstream cluster. Understanding how namespaces are use in the Fleet manager is important to understand the security model and how one can use Fleet in a multi-tenant fashion. GitRepos, Bundles, Clusters, ClusterGroups \u00b6 The primary types are all scoped to a namespace. All selectors for GitRepo targets will be evaluated against the Clusters and ClusterGroups in the same namespaces. This means that if you give create or update privileges to a the GitRepo type in a namespace, that end user can modify the selector to match any cluster in that namespace. This means in practice if you want to have two teams self manage their own GitRepo registrations but they should not be able to target each others clusters, they should be in different namespaces. Special Namespaces \u00b6 fleet-local \u00b6 The fleet-local namespace is a special namespace used for the single cluster use case or to bootstrap the configuration of the Fleet manager. When fleet is installed the fleet-local namespace is create along with one Cluster called local and one ClusterGroup called default . If no targets are specified on a GitRepo , it by default is deployed to the ClusterGroup named default , if it exists. This means that all GitRepos created in fleet-local will automatically target the local Cluster . The local Cluster refers to the cluster the Fleet manager is running on. fleet-system \u00b6 The Fleet controller and Fleet agent run in this namespace. All service accounts referenced by GitRepos are expected to live in this namespace in the downstream cluster. Cluster namespaces \u00b6 For every cluster that is registered a namespace is created by the Fleet manager for that cluster. These namespaces have are named in the form cluster-${namespace}-${cluster} . The purpose of this namespace is that all BundleDeployments for that cluster are put into this namespace and then the downstream cluster is given access to watch and update BundleDeployments in that namespace only. Cross namespace deployments \u00b6 It is possible to create a GitRepo that will deploy across namespaces. The primary purpose of this is so that a central privileged team can manage common configuration for many clusters that are managed by different teams. The way this is accomplished is by creating a BundleNamespaceMapping resource in a cluster. If you are creating a BundleNamespaceMapping resource it is best to do it in a namespace that only contains GitRepos and no Clusters . It seems to get confusing if you have Clusters in the same repo as the cross namespace GitRepos will still always be evaluated against the current namespace. So if you have clusters in the same namespace you may wish to make them canary clusters. A BundleNamespaceMapping has only two fields. Which are as below kind : BundleNamespaceMapping apiVersion : v0.3.0-alpha6 metadata : name : not-important namespace : typically-unique # Bundles to match by label. The labels are defined in the fleet.yaml # labels field or from the GitRepo metadata.labels field bundleSelector : matchLabels : foo : bar # Namespaces to match by label namespaceSelector : matchLabels : foo : bar If the BundleNamespaceMappings bundleSelector field matches a Bundles labels then that Bundle target criteria will be evaluated against all clusters in all namespaces that match namespaceSelector . One can specify labels for the created bundles from git by putting labels in the fleet.yaml file or on the metadata.labels field on the GitRepo .","title":"Namespaces"},{"location":"namespaces/#namespaces","text":"All types in the Fleet manager are namespaced. The namespaces of the manager types do not correspond to the namespaces of the deployed resources in the downstream cluster. Understanding how namespaces are use in the Fleet manager is important to understand the security model and how one can use Fleet in a multi-tenant fashion.","title":"Namespaces"},{"location":"namespaces/#gitrepos-bundles-clusters-clustergroups","text":"The primary types are all scoped to a namespace. All selectors for GitRepo targets will be evaluated against the Clusters and ClusterGroups in the same namespaces. This means that if you give create or update privileges to a the GitRepo type in a namespace, that end user can modify the selector to match any cluster in that namespace. This means in practice if you want to have two teams self manage their own GitRepo registrations but they should not be able to target each others clusters, they should be in different namespaces.","title":"GitRepos, Bundles, Clusters, ClusterGroups"},{"location":"namespaces/#special-namespaces","text":"","title":"Special Namespaces"},{"location":"namespaces/#fleet-local","text":"The fleet-local namespace is a special namespace used for the single cluster use case or to bootstrap the configuration of the Fleet manager. When fleet is installed the fleet-local namespace is create along with one Cluster called local and one ClusterGroup called default . If no targets are specified on a GitRepo , it by default is deployed to the ClusterGroup named default , if it exists. This means that all GitRepos created in fleet-local will automatically target the local Cluster . The local Cluster refers to the cluster the Fleet manager is running on.","title":"fleet-local"},{"location":"namespaces/#fleet-system","text":"The Fleet controller and Fleet agent run in this namespace. All service accounts referenced by GitRepos are expected to live in this namespace in the downstream cluster.","title":"fleet-system"},{"location":"namespaces/#cluster-namespaces","text":"For every cluster that is registered a namespace is created by the Fleet manager for that cluster. These namespaces have are named in the form cluster-${namespace}-${cluster} . The purpose of this namespace is that all BundleDeployments for that cluster are put into this namespace and then the downstream cluster is given access to watch and update BundleDeployments in that namespace only.","title":"Cluster namespaces"},{"location":"namespaces/#cross-namespace-deployments","text":"It is possible to create a GitRepo that will deploy across namespaces. The primary purpose of this is so that a central privileged team can manage common configuration for many clusters that are managed by different teams. The way this is accomplished is by creating a BundleNamespaceMapping resource in a cluster. If you are creating a BundleNamespaceMapping resource it is best to do it in a namespace that only contains GitRepos and no Clusters . It seems to get confusing if you have Clusters in the same repo as the cross namespace GitRepos will still always be evaluated against the current namespace. So if you have clusters in the same namespace you may wish to make them canary clusters. A BundleNamespaceMapping has only two fields. Which are as below kind : BundleNamespaceMapping apiVersion : v0.3.0-alpha6 metadata : name : not-important namespace : typically-unique # Bundles to match by label. The labels are defined in the fleet.yaml # labels field or from the GitRepo metadata.labels field bundleSelector : matchLabels : foo : bar # Namespaces to match by label namespaceSelector : matchLabels : foo : bar If the BundleNamespaceMappings bundleSelector field matches a Bundles labels then that Bundle target criteria will be evaluated against all clusters in all namespaces that match namespaceSelector . One can specify labels for the created bundles from git by putting labels in the fleet.yaml file or on the metadata.labels field on the GitRepo .","title":"Cross namespace deployments"},{"location":"quickstart/","text":"Quick Start \u00b6 Who needs documentation, lets just run this thing! Install \u00b6 Get helm if you don't have it. Helm 3 is just a CLI and won't do bad insecure things to your cluster. brew install helm Install the Fleet Helm charts (there's two because we separate out CRDs for ultimate flexibility.) helm -n fleet-system install --create-namespace --wait \\ fleet-crd https://github.com/rancher/fleet/releases/download/v0.3.0-alpha6/fleet-crd-0.3.0-alpha6.tgz helm -n fleet-system install --create-namespace --wait \\ fleet https://github.com/rancher/fleet/releases/download/v0.3.0-alpha6/fleet-0.3.0-alpha6.tgz Add a Git Repo to watch \u00b6 Change spec.repo to your git repo of choice. Kubernetes manifest files that should be deployed should be in /manifests in your repo. cat > example.yaml << \"EOF\" apiVersion: fleet.cattle.io/v1alpha1 kind: GitRepo metadata: name: sample # This namespace is special and auto-wired to deploy to the local cluster namespace: fleet-local spec: # Everything from this repo will be ran in this cluster. You trust me right? repo: \"https://github.com/rancher/fleet-examples\" bundleDirs: - simple EOF kubectl apply -f example.yaml Get Status \u00b6 Get status of what fleet is doing kubectl -n fleet-local get fleet You should see something like this get created in your cluster. kubectl get deploy frontend NAME READY UP-TO-DATE AVAILABLE AGE frontend 3/3 3 3 116m Enjoy and read the docs .","title":"Quick Start"},{"location":"quickstart/#quick-start","text":"Who needs documentation, lets just run this thing!","title":"Quick Start"},{"location":"quickstart/#install","text":"Get helm if you don't have it. Helm 3 is just a CLI and won't do bad insecure things to your cluster. brew install helm Install the Fleet Helm charts (there's two because we separate out CRDs for ultimate flexibility.) helm -n fleet-system install --create-namespace --wait \\ fleet-crd https://github.com/rancher/fleet/releases/download/v0.3.0-alpha6/fleet-crd-0.3.0-alpha6.tgz helm -n fleet-system install --create-namespace --wait \\ fleet https://github.com/rancher/fleet/releases/download/v0.3.0-alpha6/fleet-0.3.0-alpha6.tgz","title":"Install"},{"location":"quickstart/#add-a-git-repo-to-watch","text":"Change spec.repo to your git repo of choice. Kubernetes manifest files that should be deployed should be in /manifests in your repo. cat > example.yaml << \"EOF\" apiVersion: fleet.cattle.io/v1alpha1 kind: GitRepo metadata: name: sample # This namespace is special and auto-wired to deploy to the local cluster namespace: fleet-local spec: # Everything from this repo will be ran in this cluster. You trust me right? repo: \"https://github.com/rancher/fleet-examples\" bundleDirs: - simple EOF kubectl apply -f example.yaml","title":"Add a Git Repo to watch"},{"location":"quickstart/#get-status","text":"Get status of what fleet is doing kubectl -n fleet-local get fleet You should see something like this get created in your cluster. kubectl get deploy frontend NAME READY UP-TO-DATE AVAILABLE AGE frontend 3/3 3 3 116m Enjoy and read the docs .","title":"Get Status"},{"location":"single-cluster-install/","text":"Single Cluster Install \u00b6 In this use case you have only one cluster. The cluster will run both the Fleet manager and the Fleet agent. The cluster will communicate with Git server to deploy resources to this local cluster. This is the simplest setup and very useful for dev/test and small scale setups. This use case is supported as a valid use case for production for smaller scale. Prerequisites \u00b6 Helm 3 \u00b6 Fleet is distributed as a Helm chart. Helm 3 is just a CLI and has no server side component so it's pretty straight forward. To install the Helm 3 CLI follow the official install instructions . The TL;DR is macOS brew install helm Windows choco install kubernetes-helm Kubernetes \u00b6 Fleet is a controller running on a Kubernetes cluster so an existing cluster is required. For the single cluster use case you would install Fleet to the cluster which you intend to manage with GitOps. Any Kubernetes community supported version of Kubernetes will work, in practice this means 1.15 or greater. Install \u00b6 Install the following two Helm charts. First install the Fleet CustomResourcesDefintions. helm -n fleet-system install --create-namespace --wait \\ fleet-crd https://github.com/rancher/fleet/releases/download/v0.3.0-alpha6/fleet-crd-0.3.0-alpha6.tgz Second install the Fleet controllers. helm -n fleet-system install --create-namespace --wait \\ fleet https://github.com/rancher/fleet/releases/download/v0.3.0-alpha6/fleet-0.3.0-alpha6.tgz Fleet should be ready to use now for single cluster. You can check the status of the Fleet controller pods by running the below commands. kubectl -n fleet-system logs -l app = fleet-controller kubectl -n fleet-system get pods -l app = fleet-controller NAME READY STATUS RESTARTS AGE fleet-controller-64f49d756b-n57wq 1/1 Running 0 3m21s You can now register some git repos in the fleet-local namespace to start deploying Kubernetes resources.","title":"Single Cluster Install"},{"location":"single-cluster-install/#single-cluster-install","text":"In this use case you have only one cluster. The cluster will run both the Fleet manager and the Fleet agent. The cluster will communicate with Git server to deploy resources to this local cluster. This is the simplest setup and very useful for dev/test and small scale setups. This use case is supported as a valid use case for production for smaller scale.","title":"Single Cluster Install"},{"location":"single-cluster-install/#prerequisites","text":"","title":"Prerequisites"},{"location":"single-cluster-install/#helm-3","text":"Fleet is distributed as a Helm chart. Helm 3 is just a CLI and has no server side component so it's pretty straight forward. To install the Helm 3 CLI follow the official install instructions . The TL;DR is macOS brew install helm Windows choco install kubernetes-helm","title":"Helm 3"},{"location":"single-cluster-install/#kubernetes","text":"Fleet is a controller running on a Kubernetes cluster so an existing cluster is required. For the single cluster use case you would install Fleet to the cluster which you intend to manage with GitOps. Any Kubernetes community supported version of Kubernetes will work, in practice this means 1.15 or greater.","title":"Kubernetes"},{"location":"single-cluster-install/#install","text":"Install the following two Helm charts. First install the Fleet CustomResourcesDefintions. helm -n fleet-system install --create-namespace --wait \\ fleet-crd https://github.com/rancher/fleet/releases/download/v0.3.0-alpha6/fleet-crd-0.3.0-alpha6.tgz Second install the Fleet controllers. helm -n fleet-system install --create-namespace --wait \\ fleet https://github.com/rancher/fleet/releases/download/v0.3.0-alpha6/fleet-0.3.0-alpha6.tgz Fleet should be ready to use now for single cluster. You can check the status of the Fleet controller pods by running the below commands. kubectl -n fleet-system logs -l app = fleet-controller kubectl -n fleet-system get pods -l app = fleet-controller NAME READY STATUS RESTARTS AGE fleet-controller-64f49d756b-n57wq 1/1 Running 0 3m21s You can now register some git repos in the fleet-local namespace to start deploying Kubernetes resources.","title":"Install"},{"location":"uninstall/","text":"TODO","title":"Uninstall"}]}